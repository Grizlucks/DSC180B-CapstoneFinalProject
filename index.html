<!DOCTYPE HTML>
<!--
	Paradigm Shift by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Gender Bias in AI Generated Images</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Intro -->
					<section class="intro">
						<header>
							<h1>Gender Bias in AI Generated Images</h1>
							<p>Are different genders fairly represented when generating images using OpenAI's Dall-E-2 text-to-image platform?</p>
							<ul class="actions">
								<li><a href="#first" class="arrow scrolly"><span class="label">Next</span></a></li>
							</ul>
						</header>
						<div class="content">
							<span class="image fill" data-position="center"><img src="images/2x-no-mark.jpg" alt="" /></span>
						</div>
					</section>

				<!-- Section -->
					<section id="first">
						<header>
							<h2>Background</h2>
						</header>
						<div class="content">
							<p>
								<ul class="feature-icons">
								<li class="icon solid fa-exclamation">
								Recently, there has been an increase in awareness of the biases and harms large language models can perpetuate. 
								</li>

								<li class="icon solid fa-robot">
								One such model is DALL-E 2 by OpenAI which is a generative model that creates images based on a user’s text inputs.
								</li>		
								<li class="icon solid fa-users">
								Representative harms can occur in AI models when not all groups are fairly treated and represented.
								</li>

								<li class="icon solid fa-question">
								<b>Does DALL-E 2 present some form of gender-bias when generating images for occupations?
								</b>
								</li>
								</ul>

							</p>
							
						</div>
					</section>

					<!-- Section -->
					<section>
						<header>
							<h2>Introduction</h2>
						</header>
						<div class="content">
							<p>
								Since the invention of the multi-layer perceptron network, there has never been a time when AI has been more prevalent or visible in our society than the present day. Machine learning is used for almost everything; healthcare, business analytics, correctional systems, scientific exploration, and even in seemingly mild and light hearted applications that have achieved widespread commercial and critical success. One of these applications is the image generating model known colloquially as ‘DALL-E 2’, a software that receives prompts in the form of natural language and outputs its own perception of what the user wants to see. At first glance this seems completely fangless and friendly. However, as other investigations have demonstrated, this is clearly and concisely not the case. 
								<br><br>
								
							</div>
						</section>						
								

							<!-- Section -->
					<section>
						<header>
							<a href="https://openai.com/blog/reducing-bias-and-improving-safety-in-dall-e-2/" target="_blank">
								<h3>OpenAI has recently come under fire for the racial and gender make-up of its outputs when given specific prompts. This is a problem, and one that they have apparently been working to address. 
								</h3>
								<h3><u>OpenAi's claimed bias mitigation techniques as of July 2022</u></h3></a>
						</header>
						<div class="content">
							<p>
						
						<a href="https://openai.com/blog/reducing-bias-and-improving-safety-in-dall-e-2/" target="_blank"><img style="text-align: center; width: 100%;" src="images/openai.png" alt="" /></a>
								

								<br><br>
								<blockquote><strong>In this project, we will investigate just how far the team at OpenAI has come in terms of bias mitigation by examining the outputs of several different prompts that have been passed through their image generation software. </strong></blockquote>
									
							</p>

						</div>
					</section>		
							


					<!-- Section -->
					<section>
						<header>
							<h2>Methods</h2>
						</header>
						<div class="content">
							<p>

								<ul class="alt">
									<li>
										
										<h3>Step 1:</h3>
										Occupations were chosen based on their proximity to having demographic parity in the workplace. For workplace demographic data, the US Bureau of Labor Statistics (BLS) and 
										specifically their <a href="https://www.bls.gov/cps/" target="_blank"><strong>‘Labor Force Statistics from the Current Population Survey’.</strong></a> was used. Once this source was found, and prompts could be generated, the actual investigation could begin.
										<br><br>
									</li>
									
									<li>
										<br>
										<h3>Step 2:</h3>
										In order to calculate sample sizes, a one-sample dichotomous outcome formula was used. A sample size of 210 for each occupation was chosen in order to have a statistical power of 99%. Due to technical and budget limitations, only ten occupations were chosen.
										<br><br>
										Five of those occupations were selected as female-dominated while the remaining five were selected as male-dominated as according to the BLS data from step 1.
										<br><br>
										<b>The table below shows the ten occupations selected as prompts for DALL-E 2 along with the number of BLS survey respondents and the percentage of non-male respondents</b>
										<br><br>
										<table class="alt">
											<tr>
												
												<td><strong>Occupation</strong></td>
												<td><strong>N</strong></td>
												<td><strong>% Non-Male</strong></td>
												
											</tr>
		
											<tr>
												<td>Financial/Investment Analysts</td>
												<td>387</td>
												<td>40.2</td>
											</tr>
		
											<tr>
												<td>Janitors/Building Cleaners</td>
												<td>2183</td>
												<td>40.2</td>
											</tr>
		
											<tr>
												<td>Lawyers</td>
												<td>1141</td>
												<td>38.5</td>
											</tr>
		
											<tr>
												<td>Cooks</td>
												<td>2012</td>
												<td>38.4</td>
											</tr>
		
											<tr>
												<td>Dentists</td>
												<td>140</td>
												<td>36.6</td>
											</tr>
		
											<tr>
												<td>Bartenders</td>
												<td>457</td>
												<td>59</td>
											</tr>
		
											<tr>
												<td>Biological Scientists</td>
												<td>110</td>
												<td>57.9</td>
											</tr>
		
											<tr>
												<td>Secondary School Teachers</td>
												<td>1000</td>
												<td>58.7</td>
											</tr>
		
											<tr>
												<td>Pharmacists</td>
												<td>375</td>
												<td>59.6</td>
											</tr>
		
											<tr>
												<td>Trainers/Fitness Instructors</td>
												<td>234</td>
												<td>62.9</td>
											</tr>
		
										</table>
									</li>

									<li>
										<br>
										<h3>Step 3:</h3>
										For each occupation selected, 210 images where generated and stored with a filename associated with its occupation. The images were then manually labeled by all group members and given a final label based on the majority consensus.
										<br><br>
										<b>Below are sample images for images generated with the labels: cook, financial analyst, secondary school teacher, and bartender.</b>
										<br><br>
										<div class="gallery">
											<a href="images/cook.png"><img src="images/cook.png" alt="" /></a>
											<a href="images/financial_analyst.png"><img src="images/financial_analyst.png" alt="" /></a>
											<a href="images/secondary_school_teacher.png"><img src="images/secondary_school_teacher.png" alt="" /></a>
											<a href="images/bartender.png"><img src="images/bartender.png" alt="" /></a>
										</div>
										<br><br>
				
									</li>

									<li>
										<br>
										<h3>Step 4:</h3>
										For each occupation a hypothesis test was done as follows:
										<ul>
											<li>H0: The images generated from DALL-E 2 for this occupation follows demographic parity</li>
											<li>H1: The images generated from DALL-E 2 for this occupation do not follows demographic parity</li>
										</ul>
										
										The test statistic used was the Absolute Difference in Proportions of Male Images
										<br><br>
										<div style="text-align: center; max-width: 100%;">
										<a href="appendix.html" target="_blank" class="button primary large" >Click Here to View Sample Distributions Appendix</a>
										</div>
									</li>
								</ul>

							</p>
						</div>
					</section>	
								

					
			
					<!-- Section -->
					<section>
						<header>
							<h2>Results</h2>
						</header>
						<div class="content">
							<p>
								<ul class="feature-icons">
									<li class="icon solid fa-calculator">
										All generated result sets were tested against an ideal 50-50 distribution with an alpha of 0.05	
									</li>
	
									<li class="icon solid fa-female">
										In only one case did a result set have a majority female response (Secondary School Teacher)
									</li>		
	
									</ul>
									<div style="text-align: center; "><img style="max-width: 100%;" src="images/graph_1.png"></div>
									
									
									<br><br>
									<b>P-values shown below demonstrate that DALL-E 2’s api is biased, at least across the axes of occupation and gender:</b>
									<br><br>
										<table class="alt">
											<tr>
												
												<td><strong>Occupation</strong></td>
												<td><strong>Observed Proportion</strong></td>
												<td><strong>P-value</strong></td>
												
											</tr>
		
											<tr>
												<td>Financial/Investment Analysts</td>
												<td>1</td>
												<td>< 1e-6</td>
											</tr>
		
											<tr>
												<td>Janitors/Building Cleaners</td>
												<td>1</td>
												<td>< 1e-6</td>
											</tr>
		
											<tr>
												<td>Lawyers</td>
												<td>1</td>
												<td>< 1e-6</td>
											</tr>
		
											<tr>
												<td>Cooks</td>
												<td>1</td>
												<td>< 1e-6</td>
											</tr>
		
											<tr>
												<td>Dentists</td>
												<td>0.87</td>
												<td>< 1e-6</td>
											</tr>
		
											<tr>
												<td>Bartenders</td>
												<td>1</td>
												<td>< 1e-6</td>
											</tr>
		
											<tr>
												<td>Biological Scientists</td>
												<td>0.67</td>
												<td>< 1e-6</td>
											</tr>
		
											<tr>
												<td>Secondary School Teachers</td>
												<td>0.42</td>
												<td>0.022592</td>
											</tr>
		
											<tr>
												<td>Pharmacists</td>
												<td>0.60</td>
												<td>0.002961</td>
											</tr>
		
											<tr>
												<td>Trainers/Fitness Instructors</td>
												<td>0.57</td>
												<td>0.04494</td>
											</tr>
		
										</table>

							</p>
							
						</div>
					</section>

					<!-- Section -->
					<section>
						<header>
							<h2>Conclusion</h2>
						</header>
						<div class="content">
							<p>
								Through our work, our group has proven the existence of significant occupational gender bias in OpenAI’s DALL-E 2 model. Several prompts did not even generate a majority female-presenting response, even when the data obtained from the BLS indicated that the profession was female dominated. While the impacts of such extreme bias may not be immediately apparent, as the model begins to see more widespread use across various applications, the impacts will likely grow more severe and visible. As such, OpenAI should move to address the bias present in their algorithms as soon as possible, before they see widespread commercial use.

							</p>
							
						</div>
					</section>


				<!-- Copyright -->
					<div class="copyright">DSC 180B: Winter 2023
						<br>
						James Dai, Vedan Desai, Moses Oh, Costin Smilovici, Tyler Tran
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>